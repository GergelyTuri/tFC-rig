"""Check, prep, and clean files prior to running an analysis.

The `files` module interacts with the data folders and files, with
public methods `check`, `prep`, `clean` that prepare the data for an
analysis. These methods were developed specific to the original data
collection efforts, and therefore reflect the types of mistakes common
at that time. The `RigFiles` class should be setup with `dry_run` set
to `True` prior to actually modifying data to ensure the changes it
will make are safe and expected.
"""

import json
import os
import re
import shutil
from copy import deepcopy
from dataclasses import dataclass
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from tfcrig import (
    create_cohort_pattern,
    extract_cohort_mouse_pairs,
    root_contains_cohort_of_interest,
    DATETIME_REGEX
)
from tfcrig.notebook import builtin_print

BAD_DATE_REGEX_1 = r"(\d{1,2})[ \/](\d{1,2})[ \/](\d{2,4})"
"""
Possible date format in Google Drive folder names
"""
BAD_DATE_REGEX_2 = r"(\d{1,2})[_](\d{1,2})[_](\d{2,4})"
"""
Possible date format in Google Drive folder names
"""

FILENAME_REGEX = DATETIME_REGEX + r".json"


def is_base_data_file(file_name: str) -> bool:
    """
    Given a file name, determine whether it is an base data file.
    These are JSON files used to build the data frame for the analysis.
    They are not the raw, analyzed, or processed data that may be used
    as an intermediate step in the analysis
    """
    if re.search(FILENAME_REGEX, file_name):
        return True
    return False


@dataclass
class RigFiles:
    """
    Given an absolute file path to a set of project data generated by
    a trace fear conditioning rig, provide a set of helper functions to
    clean the data before an analysis.
    """

    data_root: str = "/gdrive/Shareddrives/Turi_lab/Data/aging_project/"
    dry_run: bool = True
    verbose: bool = False
    cohorts: list[str] = None

    def __post_init__(self):
        """
        Because this is a dataclass, use a post-init method to do
        what you might normally do in init
        """
        self.cohort_pattern = create_cohort_pattern(self.data_root)

        # Need to pre-define the set of directories and files of interest
        self.os_walk = []
        for root, dirs, files in os.walk(self.data_root):
            if root_contains_cohort_of_interest(
                root, self.cohort_pattern, self.cohorts
            ) or not self.cohorts:
                self.os_walk.append((root, dirs, files))

    def check(self) -> None:
        """
        Performs a set of checks. The `clean` public method on this
        class would, when applicable, fix what these checks discover
        """
        self._are_data_files_named_correctly()

    def prep(self) -> None:
        """
        Prep the data directory for cleaning. This at least means
        making a `raw` copy of the data
        """
        self._make_a_copy_of_raw_data()

    def clean(self) -> None:
        """
        Runs through a set of ways to clean the data. If `dry_run` is
        `True` it will only print out what it _would_ have cleaned,
        which can be useful as a safety check before modifying any
        data
        """
        self._rename_some_bad_file_name_patterns()
        self._rename_date_directories()
        self._examine_and_fix_typos_in_data_files()

    def sync(self) -> None:
        """
        Data is collected with a setup that runs two mice through the
        protocol at the same time on a primary and a secondary rig. The
        time at which certain fixed events occur between files can be
        offset; this method attempts to correct for that offset. It
        checks that the offset for each file is within a certain
        threshold that is deemed acceptable to correct for behavioral
        data.
        """
        self._sync_messages_to_second_mouse()
        self._sync_primary_secondary_rigs()

    def restore(self) -> None:
        """
        Restore data directories to their original states by removing
        processed files. This will result in processed data removal!
        Highly recommended to run this with `dry_run` and `verbose` set
        to `True` first to ensure the correct files are removed.
        """
        folder_exceptions = [
            self.data_root,
        ]
        self._remove_processed_data(
            files_to_remove=["_raw.json", ".pdf"],
            folder_exceptions=folder_exceptions,
        )


    @staticmethod
    def reformat_date_in_directory(directory: str) -> str:
        """
        Given a string that might represent a date based on the above
        regex, return a consistently formatted date string `YYYY_MM_DD`
        """
        if not isinstance(directory, str):
            raise TypeError(
                f"Can only reformat strings!"
            )

        if not directory:
            raise ValueError(
                f"Can only reformat non-empty strings!"
            )

        re_date = re.match(BAD_DATE_REGEX_1, directory)
        if not re_date:
            # Not feeling great about this logic
            re_date = re.match(BAD_DATE_REGEX_2, directory)

        if not re_date:
            # Only reformat dates if they match the expected pattern.
            # Do nothing otherwise
            return directory

        month, day, year = re_date.groups()

        if len(year) == 2:
            # Assumes we only process dates from the year 2000 on
            year = "20" + year
        month = month.zfill(2)
        day = day.zfill(2)

        return f"{year}_{month}_{day}"

    def _are_data_files_named_correctly(self) -> None:
        """
        Crawls the entire data root directory and checks the originally
        uploaded file names. No corrections are made here
        """
        builtin_print("")
        print("Checking data file names for consistency!")

        data_files = set()
        close_data_files = set()
        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue

                file_name_parts = re.split(DATETIME_REGEX, file_name)
                cohort_mouse_pairs = extract_cohort_mouse_pairs(file_name_parts[0])
                if not cohort_mouse_pairs or any(
                    "-" in mouse_id for mouse_id in cohort_mouse_pairs
                ):
                    # Some JSON files are named incorrectly, or they contain
                    # the correct date-time string but no information on the
                    # experiment or mouse
                    close_data_files.add(os.path.join(root, file_name))
                    continue

                # These should be good. But, check them!
                data_files.add(os.path.join(root, file_name))

        # Print a wrap-up of the output
        if data_files and self.verbose:
            print("Correctly named data files:")
            for f in data_files:
                builtin_print(f"  {f}")
        if close_data_files:
            print("Potential data files, incorrectly named:")
            for f in close_data_files:
                builtin_print(f"  {f}")
        else:
            print("No potential bad data files found!")

    def _make_a_copy_of_raw_data(self) -> None:
        """
        Given a data directory, make a copy of each JSON file in the
        directory, as long as:

            - The file does not end with `_raw.json`
            - The file does not already have a copy

        Only the originally-uploaded JSON files should be copied.
        """
        builtin_print("")
        print("Creating copies of raw data files!")

        count = 0
        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue
                full_file = os.path.join(root, file_name)

                # Define the raw file path and see if it exists
                file_parts = file_name.split(".")
                raw_full_file = os.path.join(root, file_parts[0] + "_raw.json")
                if os.path.exists(raw_full_file):
                    continue

                # Make a copy of the file
                print(f"Created a copy of {full_file}")
                shutil.copy(full_file, raw_full_file)
                count += 1
        if not count:
            print(f"No copies created!")

    def _rename_some_bad_file_name_patterns(self) -> None:
        """
        Renames files with incorrect naming patterns. This should have
        only been necessary prior to enforcing file name patterns using
        the GUI, where there were some inconsistencies:

            - Mouse IDs or files prefixed with 'mouse_'
            - Mouse IDs contain '-' instead of '_'

        Args:
            None

        Returns:
            None
        """
        builtin_print("")
        print("Renaming some incorrectly named files!")

        full_files_to_fix = []
        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue
                bad_file = os.path.join(root, file_name)
                better_file = ""

                # Remove the `mouse_` prefix from some files
                prefix = "mouse_"
                if file_name.startswith(prefix):
                    better_file = os.path.join(root, file_name.split(prefix)[-1])

                # The `cohort_mouse_blob` may accidentally have a `-` instead of `_`
                file_name_parts = re.split(DATETIME_REGEX, file_name)
                cohort_mouse_blob = file_name_parts[0]
                bad_char = "-"
                if bad_char in cohort_mouse_blob:
                    rest_of_name = file_name.split(cohort_mouse_blob)[-1]
                    cohort_mouse_blob = cohort_mouse_blob.replace(bad_char, "_")
                    better_file_name = cohort_mouse_blob + rest_of_name
                    better_file = os.path.join(root, better_file_name)

                # If no `better_file` is defined, we haven't detected
                # anything to fix and can continue
                if not better_file:
                    continue

                # We have a `bad_file` and `better_file`, but this `clean`
                # method runs after we've created a copy of the raw data. See
                # if a `bad_raw_file` exists
                bad_raw_file = bad_file.replace(".json", "_raw.json")
                if not os.path.isfile(bad_raw_file):
                    raise FileNotFoundError(
                        f"Raw file corresponding to {bad_file} not found. "
                        "The `prep` method should be run on all files before "
                        "the `clean` method."
                    )
                better_raw_file = better_file.replace(".json", "_raw.json")

                # Store the changes we would make for later
                full_files_to_fix += [
                    (bad_file, better_file),
                    (bad_raw_file, better_raw_file)
                ]

        # Fix the files
        for files in full_files_to_fix:
            if self.dry_run:
                print("Bad file found.")
                builtin_print("  Would rename:")
                builtin_print(f"    {files[0]}")
                builtin_print("  To:")
                builtin_print(f"    {files[1]}")
            else:
                os.rename(files[0], files[1])

        # Summarize
        if not full_files_to_fix:
            print("Found no bad file names!")
        else:
            n = len(full_files_to_fix)
            if self.dry_run:
                print(f"Would fix {n} bad file names.")
            else:
                print(f"Fixed {n} bad file names!")

    def _rename_date_directories(self) -> None:
        """
        Method to rename the dates in data directories to be consistent
        """
        builtin_print("")
        print("Renaming some incorrectly named directories!")

        count = 0
        for root, directories, _ in self.os_walk:
            for directory in directories:
                bad_dir_match_1 = re.match(BAD_DATE_REGEX_1, directory)
                bad_dir_match_2 = re.match(BAD_DATE_REGEX_2, directory)

                need_to_fix_dir_name = False
                if bad_dir_match_1:
                    # We have a date like `3 25 24` or `3/25/24`
                    need_to_fix_dir_name = True
                elif bad_dir_match_2:
                    # We have a date like `3_25_24`
                    need_to_fix_dir_name = True

                if not need_to_fix_dir_name:
                    continue
                count += 1

                bad_path = os.path.join(root, directory)
                better_path = os.path.join(
                    root, self.reformat_date_in_directory(directory)
                )
                if self.dry_run:
                    print("Bad directory found.")
                    builtin_print("  Would rename:")
                    builtin_print(f"    {bad_path}")
                    builtin_print("  To:")
                    builtin_print(f"    {better_path}")
                else:
                    os.rename(bad_path, better_path)

        if not count:
            print("Found no bad directory names!")

    def _examine_and_fix_typos_in_data_files(self) -> None:
        """
        Correct some typos in the data
        """
        builtin_print("")
        print("Fixing some incorrect data, be careful!")

        count = 0
        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue

                need_to_fix_data = False
                need_to_fix_msg = ""
                file_name_parts = re.split(DATETIME_REGEX, file_name)
                cohort_mouse_pairs = extract_cohort_mouse_pairs(file_name_parts[0])
                mouse_ids = [e[0:-1] for e in cohort_mouse_pairs]

                file_path = os.path.join(root, file_name)
                with open(file_path, "r") as f:
                    data = json.load(f)

                # Some old files, or perhaps any experiment with only a single mouse,
                # has a `mouse_id` header value when they should all be `mouse_ids`
                if "mouse_id" in data["header"]:
                    mouse_id = data["header"]["mouse_id"]
                    del data["header"]["mouse_id"]
                    data["header"]["mouse_ids"] = [mouse_id]
                    need_to_fix_msg = "'mouse_id' in header"
                    need_to_fix_data = True

                # Some mouse IDs have the incorrect format, either by starting with
                # `mouse_` or by containing "-" instead of "_"
                mouse_ids = []
                for mouse_id in data["header"]["mouse_ids"]:
                    modify_mouse_id = False
                    original_mouse_id = deepcopy(mouse_id)

                    # Fix ID that starts with `mouse_`
                    if mouse_id.startswith("mouse_"):
                        mouse_id = mouse_id.split("mouse_")[-1]
                        modify_mouse_id = True

                    # Fix ID that contains `-`
                    if "-" in mouse_id:
                        mouse_id = mouse_id.replace("-", "_")
                        modify_mouse_id = True

                    # Fix every instance of the mouse ID. This does assume the ID is unique
                    # within the JSON blob
                    if modify_mouse_id:
                        data_str = json.dumps(data)
                        data_str = data_str.replace(
                            f'"{original_mouse_id}"', f'"{mouse_id}"'
                        )
                        data = json.loads(data_str)
                        need_to_fix_msg = "'mouse_id' format is bad"
                        need_to_fix_data = True

                    mouse_ids.append(mouse_id)

                # Now we have the correct `mouse_ids`, we check the format of `data`, it
                # should map to a dictionary per `mouse_id`, but if there is only one
                # `mouse_id` we need to modify it
                if len(data["header"]["mouse_ids"]) == 1 and isinstance(
                    data["data"], list
                ):
                    data["data"] = {data["header"]["mouse_ids"][0]: data["data"]}
                    need_to_fix_msg = "single mouse 'data' modification"
                    need_to_fix_data = True

                # Sometimes, we include a `mesage` key instead of `message`, it isn't
                # clear from a data analysis perspective why
                data_str = json.dumps(data)
                if '"mesage"' in data_str:
                    data_str = data_str.replace('"mesage"', '"message"')
                    data = json.loads(data_str)
                    need_to_fix_msg = "'mesage' in data"
                    need_to_fix_data = True
                del data_str

                # Fix individual data blobs.
                # This is based on the output from setting up the analysis
                def is_good_data_blob(gui_msg: dict) -> bool:
                    # The GUI message itself can be bad
                    try:
                        rig_msg = gui_msg["message"].strip()
                    except KeyError as e:
                        if "KeyboardInterrupt" in gui_msg:
                            return False
                        raise e

                    # Is the message not formatted properly?
                    split_msg = rig_msg.split(": ")
                    n_chunks = len(split_msg)
                    if n_chunks not in [4, 5]:
                        return False
                    try:
                        split_ints = [
                            int(split_msg[0]),
                            int(split_msg[1]),
                            int(split_msg[2]),
                        ]
                    except ValueError:
                        # The first three message parts are integers
                        return False

                    # Known good messages that can be false positives for the
                    # below known bad
                    if split_msg[3] == "Waiting for session to start...":
                        return True

                    # Removes at least some known bad messages
                    if rig_msg == "KeyboardInterrupt":
                        # Keyboard interrupts appear as a rig message
                        return False
                    if rig_msg[-10::] in "Waiting for session to start...":
                        # For some reason, portions of this message appear in
                        # the data often
                        return False
                    if "Waiting for session to start..." in rig_msg:
                        # Might be the same weirdness as above
                        return False
                    if (
                        rig_msg
                        in "Your session has ended, but a sketch cannot stop Arduino."
                    ):
                        # Something the rig prints that we can ignore
                        return False
                    if "Session consists of " in rig_msg:
                        # Known non-conforming string
                        return False

                    return True

                for mouse_id in list(data["data"].keys()):
                    original_mouse_data = data["data"][mouse_id]
                    new_mouse_data = [
                        blob for blob in original_mouse_data if is_good_data_blob(blob)
                    ]
                    data["data"][mouse_id] = new_mouse_data

                    try:
                        diff = [
                            x["message"]
                            for x in original_mouse_data
                            if x not in new_mouse_data
                        ]
                    except Exception:
                        diff = [
                            x for x in original_mouse_data if x not in new_mouse_data
                        ]
                    if diff:
                        need_to_fix_data = True
                        need_to_fix_msg = "Bad data blob removed"

                # Fix the data
                if need_to_fix_data:
                    print(f"Fixing '{file_path}'")
                    builtin_print(f"- Reason: {need_to_fix_msg}")
                    count += 1
                    if not self.dry_run:
                        with open(file_path, "w") as f:
                            json.dump(data, f, indent=4)

        if count == 0:
            print("Did not need to fix any files!")
        else:
            print(f"Fixed {count} files!")

    def _sync_primary_secondary_rigs(self) -> None:
        """
        Correct file timestamps between the primary and secondary rigs
        """
        builtin_print("")
        print("Syncing timestamps between the primary and secondry rigs!")

        # These messages should occur at the same time for both the primary and
        # secondary mouse. They'll be used to calculate primary-secondary rig
        # offset, and then the trial starts will be used  as the anchor point
        # for syncing data between mice
        trial_start_msg = "Trial has started"
        trial_end_msg = "Trial has ended"

        # These messages might be missing from one or the other mouse data
        # sets. They can be added to the data sets they are missing from
        missing_messages = [
            'Puff start"',
            'Puff stop"',
            'Puff stop, catch block"',
            'Negative signal start"',
            'Negative signal stop"',
            'Positive signal start"',
            'Positive signal stop"',
        ]

        # This checks that every file can be opened. It is repetitive code from
        # below, but because the sync process takes time while developing and
        # there might be an error in opening a newly uploaded file, this is
        # kept here for now
        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue

                full_file = os.path.join(root, file_name)
                with open(full_file, "r") as f:
                    data = json.load(f)

        start_time_offset_data = []
        start_time_offsets = []
        sample_files = True
        skip = 0
        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue
                full_file = os.path.join(root, file_name)

                # Skip every nth file, to sample across all cohorts but save
                # time running the sync. Set `sample_files` to `False` to sync
                # all files
                skip += 1
                if sample_files and skip % 10 != 0:
                    continue

                # Work only with files with two mice
                file_name_parts = re.split(DATETIME_REGEX, file_name)
                cohort_mouse_pairs = extract_cohort_mouse_pairs(file_name_parts[0])
                mouse_ids = [e[0:-1] for e in cohort_mouse_pairs]
                if len(mouse_ids) == 1:
                    continue
                if len(mouse_ids) > 2:
                    raise ValueError(
                        f"File {full_file} has more than two mouse IDs! This "
                        "method can only sync a primary and secondary rig."
                    )
                if mouse_ids[0] == mouse_ids[1]:
                    raise ValueError(
                        f"File {full_file} has two mouse IDs, but they are "
                        "the same mouse!"
                    )

                # Check that the file is consistent. This isn't necessary as it
                # is likely done elsewhere, but is added while testing
                with open(full_file, "r") as f:
                    data = json.load(f)
                stored_mouse_ids = data["header"]["mouse_ids"]
                if mouse_ids != stored_mouse_ids:
                    # TODO: this should be a raised `ValueError` but it just
                    # continues until the existing data errors are fixed
                    print(
                        f"File {full_file} has mouse IDs in its file name "
                        "that do not match the file header data."
                    )
                    continue

                # Get the `Trial has started` messages which'll be used as the
                # anchor point to sync data between the primary and secondary
                # mice
                first_trial_starts = [
                    entry
                    for entry in data["data"][mouse_ids[0]]
                    if trial_start_msg in entry["message"]
                ]
                second_trial_starts = [
                    entry
                    for entry in data["data"][mouse_ids[1]]
                    if trial_start_msg in entry["message"]
                ]
                if len(first_trial_starts) != len(second_trial_starts):
                    raise ValueError(
                        f"File {full_file} has a first and second mouse whose "
                        "data have a different number of trials based on the "
                        "trial start times!"
                    )

                # Get the `Trial has ended` messages which'll be used to help
                # calculate the offset between the first and second mice
                first_trial_ends = [
                    entry
                    for entry in data["data"][mouse_ids[0]]
                    if trial_end_msg in entry["message"]
                ]
                second_trial_ends = [
                    entry
                    for entry in data["data"][mouse_ids[1]]
                    if trial_end_msg in entry["message"]
                ]
                if len(first_trial_ends) != len(second_trial_ends):
                    raise ValueError(
                        f"File {full_file} has a first and second mouse whose "
                        "data have a different number of trials based on the "
                        "trial end times!"
                    )

                # Ensure that the number of starts and ends is the same, and
                # that they are not empty
                if len(first_trial_starts) != len(first_trial_ends):
                    # TODO: this should be a raised `ValueError` but it just
                    # continues until the existing data errors are fixed
                    print(
                        f"File {full_file} has a first mouse whose number of "
                        "trial start and end times differs."
                    )
                    continue
                if not first_trial_starts:
                    # TODO: this should be a raised `ValueError` but it just
                    # continues until the existing data errors are fixed
                    print(
                        f"File {full_file} does not have any trial starts!"
                    )
                    continue

                # Ensure that the trials all start at trial start time zero
                trial_start_times = [
                    int(entry["message"].split(":")[2].strip())
                    for entry in first_trial_starts + second_trial_starts
                ]
                if not all(t_start == 0 for t_start in trial_start_times):
                    raise ValueError(
                        f"File {full_file} has a trial that does not start at "
                        "trial start time zero."
                    )

                # Trial time should not be impacted by a rig offset. Confirm
                # that the mean of the second mouse trial end times is within
                # one standard deviation of the first trial end times, or 10
                # millis, of the mean of the first mouse trial end times. The
                # 10 millis distinction is made because trial end times can be
                # exactly the same (e.g. 50,001) for all trials, leading to a
                # zero standard deviation, but checking the standard deviation
                # accounts for if trial end times ever do vary between trials
                first_trial_end_times = [
                    int(entry["message"].split(":")[2].strip())
                    for entry in first_trial_ends
                ]
                second_trial_end_times = [
                    int(entry["message"].split(":")[2].strip())
                    for entry in second_trial_ends
                ]
                mu_one = np.mean(first_trial_end_times)
                std_one = np.std(first_trial_end_times)
                mu_two = np.mean(second_trial_end_times)
                if abs(mu_one-mu_two) > max(std_one, 10):
                    raise ValueError(
                        f"File {full_file} has a second mouse whose trial end "
                        "times differ significantly from its first mouse's "
                        "trial end times."
                    )

                # Get an array of absolute trial start times for the first and
                # second mouse, and look at their diff (Unix time, which is in
                # seconds, is used)
                abs_first_trial_start_times = np.array([
                    datetime.strptime(
                        entry["absolute_time"], "%Y-%m-%d_%H-%M-%S.%f"
                    ).timestamp()
                    for entry in first_trial_starts
                ])
                abs_second_trial_start_times = np.array([
                    datetime.strptime(
                        entry["absolute_time"], "%Y-%m-%d_%H-%M-%S.%f"
                    ).timestamp()
                    for entry in second_trial_starts
                ])
                abs_trial_start_diff = abs_first_trial_start_times - abs_second_trial_start_times
                start_time_offsets += abs(abs_trial_start_diff).tolist()
                mu = round(1000*np.mean(abs_trial_start_diff), 0)
                std = round(1000*np.std(abs_trial_start_diff), 0)
                start_time_offset_data.append((mu, std))

                # We want to iterate over the first mouse data, find
                # potentially missing messages, see if they occur in the second
                # mouse data and insert them if they don't
                mouse_one_potential = []
                for entry in data["data"][mouse_ids[0]]:
                    # Collect all the potentially missing messages
                    msg = entry["message"].split(":")[-1].strip()
                    if msg in missing_messages:
                        if entry["mouse_id"] != mouse_ids[0]:
                            raise ValueError("Bad mouse id!")
                        mouse_one_potential.append(entry)
                add_to_mouse_two_entries = []
                for mouse_one_entry in mouse_one_potential:
                    # For each potentially missing mouse message, see if it has
                    # a match in the second mouse data
                    msg_one = mouse_one_entry["message"].split(":")[-1].strip()
                    t_one = datetime.strptime(
                        mouse_one_entry["absolute_time"], "%Y-%m-%d_%H-%M-%S.%f"
                    ).timestamp()

                    found_match = False
                    for entry in data["data"][mouse_ids[1]]:
                        msg_two = entry["message"].split(":")[-1].strip()
                        t_two = datetime.strptime(
                            entry["absolute_time"], "%Y-%m-%d_%H-%M-%S.%f"
                        ).timestamp()
                        if msg_one == msg_two:
                            if abs(t_one-t_two) < 0.5:
                                found_match = True
                    if not found_match:
                        add_to_mouse_two_entries.append(mouse_one_entry)

                if add_to_mouse_two_entries:
                    raise ValueError(
                        f"File {file_name} did not have messages synced correctly"
                    )

                # Check the count of each potentially missing message, in the
                # data set for each mouse
                first_missing_count = []
                second_missing_count = []
                for msg in missing_messages:
                    first_count = [
                        entry
                        for entry in data["data"][mouse_ids[0]]
                        if msg in entry["message"]
                    ]
                    second_count = [
                        entry
                        for entry in data["data"][mouse_ids[1]]
                        if msg in entry["message"]
                    ]
                    first_missing_count.append(len(first_count))
                    second_missing_count.append(len(second_count))
                first_missing_count = np.array(first_missing_count)
                second_missing_count = np.array(second_missing_count)
                diff_missing_count = first_missing_count - second_missing_count
                if any(diff_missing_count):
                    print(f"File {file_name} has unsynced missing messages")
                    print(f"{file_name}: {diff_missing_count}")

        # Show a histogram of the offset data.
        # This can tell us, for the given data, how spread the offset
        # is. It won't tell us whether the offset drifts throughout a
        # given session
        start_time_offsets = [
            round(1000*x, 0)
            for x in start_time_offsets
        ]
        sns.histplot(np.array(start_time_offsets), bins=50, kde=True, color="blue")
        plt.xlabel("Milliseconds")
        plt.ylabel("Frequency")
        plt.title("Primary, Secondary Trial Start Time Offset")
        plt.show()

    def _sync_messages_to_second_mouse(self) -> None:
        """
        Add exact puff, positive signal, and negative signal timings
        to second mouse data
        """
        print("Syncing puffs and signals from first mouse to second mouse")

        missing_messages = [
            "Puff start",
            "Puff stop",
            "Puff stop, catch block",
            "Negative signal start",
            "Negative signal stop",
            "Positive signal start",
            "Positive signal stop",
        ]
        missing = False

        for root, _, files in self.os_walk:
            for file_name in files:
                if not is_base_data_file(file_name):
                    continue

                # This specific file needs to be skipped but is an
                # outlier. When it was created, the hard drive on the
                # computer running the rig ran out of space
                if file_name == "106_3_106_4_2025-01-17_13-44-11.json":
                    continue

                full_file = os.path.join(root, file_name)

                with open(full_file, "r") as f:
                    data = json.load(f)
                    mouse_ids = data["header"]["mouse_ids"]
                # Ensure we have two mice in the data
                if len(mouse_ids) == 2:
                    first_mouse_id = mouse_ids[0]
                    second_mouse_id = mouse_ids[1]
                    sec_port = data["header"]["secondary_port"]

                    has_required_messages = any(
                        any(
                            entry["message"].strip().endswith(msg)
                            for msg in missing_messages
                        )
                        for entry in data["data"][second_mouse_id]
                    )
                    if not has_required_messages:
                        missing = True

                        # Calculate time difference due to delay in second rig
                        first_mouse_trial_end_times = [
                            pd.to_datetime(
                                entry["absolute_time"], format="%Y-%m-%d_%H-%M-%S.%f"
                            )
                            for entry in data["data"][first_mouse_id]
                            if entry["message"].strip().endswith("Trial has ended")
                        ]

                        second_mouse_trial_end_times = [
                            pd.to_datetime(
                                entry["absolute_time"], format="%Y-%m-%d_%H-%M-%S.%f"
                            )
                            for entry in data["data"][second_mouse_id]
                            if entry["message"].strip().endswith("Trial has ended")
                        ]
                        first_mouse_trial_end_times = sorted(
                            first_mouse_trial_end_times
                        )
                        second_mouse_trial_end_times = sorted(
                            second_mouse_trial_end_times
                        )

                        # Ensure trials are synced based on trial starts
                        combined_data = data["data"][second_mouse_id]
                        prev_end_time = None
                        for i, first_end_time in enumerate(first_mouse_trial_end_times):
                            if i < len(second_mouse_trial_end_times):
                                second_end_time = second_mouse_trial_end_times[i]
                                time_diff = first_end_time - second_end_time

                                first_mouse_trial_messages = [
                                    {
                                        "message": entry["message"],
                                        "mouse_id": second_mouse_id,
                                        "port": sec_port,
                                        "absolute_time": (
                                            pd.to_datetime(
                                                entry["absolute_time"],
                                                format="%Y-%m-%d_%H-%M-%S.%f",
                                            )
                                            - time_diff
                                        ).strftime("%Y-%m-%d_%H-%M-%S.%f"),
                                    }
                                    for entry in data["data"][first_mouse_id]
                                    if any(
                                        entry["message"].strip().endswith(msg)
                                        for msg in missing_messages
                                    )
                                    and pd.to_datetime(
                                        entry["absolute_time"],
                                        format="%Y-%m-%d_%H-%M-%S.%f",
                                    )
                                    <= first_end_time
                                    and (
                                        prev_end_time is None
                                        or pd.to_datetime(
                                            entry["absolute_time"],
                                            format="%Y-%m-%d_%H-%M-%S.%f",
                                        )
                                        > prev_end_time
                                    )
                                ]
                                # Sync the data with corrected times for the second mouse
                                combined_data.extend(first_mouse_trial_messages)
                                prev_end_time = first_end_time
                        combined_data.sort(
                            key=lambda entry: pd.to_datetime(
                                entry["absolute_time"], format="%Y-%m-%d_%H-%M-%S.%f"
                            )
                        )
                        data["data"][second_mouse_id] = combined_data

                        # TESTING purposes - write to a new file
                        # dir_name, base_name = os.path.split(full_file)
                        # file_name, file_extension = os.path.splitext(base_name)
                        # new_file_name = f"{file_name}_v2{file_extension}"
                        # new_file_path = os.path.join(dir_name, new_file_name)
                        # with open(new_file_path, "w") as f:
                        # json.dump(data, f, indent=4)
                        with open(full_file, "w") as f:
                            json.dump(data, f, indent=4)

        if missing:
            print("Filled and synced missing data for second mouse!")


    def _remove_processed_data(
        self,
        directories_to_clean=[],
        files_to_remove=["_raw.json", ".pdf"],
        folder_exceptions=None,
    ) -> None:
        """
        Given a list of directories, removes files with specified endings, excluding critical folders.

        Args:
            directories_to_clean: List of directories where the action should take place.
            files_to_remove: List of file endings to remove.
            folder_exceptions: List of folder paths to exclude from the operation.
        """
        if folder_exceptions is None:
            folder_exceptions = []  # Default to an empty list if no folder exceptions are provided

        if self.verbose:
            print(
                f"Starting file removal. Directories to clean: {', '.join(directories_to_clean)}"
            )
            print(f"Excluding folders: {', '.join(folder_exceptions)}")

        count = 0
        files_to_be_removed = []  # Collect files to be removed for confirmation

        for directory in directories_to_clean:
            # Ensure the directory exists before processing
            if not os.path.exists(directory):
                print(f"Directory does not exist: {directory}")
                continue

            for root, _, files in os.walk(directory):
                # Check if current directory is in the folder exceptions
                if any(
                    os.path.commonpath([root, exc]) == exc for exc in folder_exceptions
                ):
                    if self.verbose:
                        print(f"Skipping folder: {root}")
                    continue

                for file in files:
                    full_file = os.path.join(root, file)
                    # Check if file ends with any of the specified endings
                    if any(full_file.endswith(ending) for ending in files_to_remove):
                        files_to_be_removed.append(full_file)
                        if self.verbose:
                            print(f"Matched for removal: {full_file}")

        if not files_to_be_removed:
            print("No files matched the criteria.")
            return

        # Confirmation prompt
        print(f"\n{len(files_to_be_removed)} files matched the criteria for removal.")
        for f in files_to_be_removed[:5]:  # Show up to 5 matched files
            print(f"- {f}")
        if len(files_to_be_removed) > 5:
            print(f"...and {len(files_to_be_removed) - 5} more.")

        confirmation = (
            input("Do you want to proceed with deletion? (y/n): ").strip().lower()
        )
        if confirmation != "y":
            print("Operation cancelled. No files were removed.")
            return

        # Proceed with removal
        for full_file in files_to_be_removed:
            count += 1
            if self.dry_run:
                if self.verbose:
                    print(f"[DRY RUN] Would remove: {full_file}")
            else:
                os.remove(full_file)
                if self.verbose:
                    print(f"Removed: {full_file}")

        if self.verbose:
            print(f"Total files removed: {count}")
